{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Image alignment and registration\n\n\n**Stephen Jesse, Alex Belianinov, Suhas Somnath**\n\n* Institute for Functional Imaging of Materials\n* Center for Nanophase Materials Sciences\n\nOak Ridge National Laboratory, Oak Ridge TN 37831, USA\n\n7/29/2015\n\nOften scientists find themselves wanting to compare data of various origins on the same sample that has a location of\ninterest where multiple experiments have been carried out. Often, these data sets may not have the same resolution, not\nhave been captured over the exact same region, be distorted in different ways due to drift, and even have different\ndimensionality. In this example, we will make use of algorithms which attempt to find the best alignment transformation\nbetween data sets.\n\nBelow, the Normalized Topo Image is a result of STM (scanning tunneling microscope) imaging captured prior to\ncollecting a lower spatial resolution, 3D scanning tunneling spectroscopic (STS) data set that experienced drift as\nis evident shown in Z Channel Spectroscopy (where Z is height roughly corresponding to topography). We would like to\nmap the spectroscopic result onto exact locations in the 2D map (high resolution) to correlate topo features to the\nelectronic effects captured by STS. To do this, the image (and the associated full 3D data set) on the right needs to\nbe transformed to look like the image on the left.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gather our tools and load necessary libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division, unicode_literals  # ensure python3 compatibility\nimport numpy as np  # fast math\nfrom warnings import warn\nimport matplotlib.pyplot as plt  # plotting\nimport h5py  # reading the data file\nimport os  # file operations\nfrom scipy import interpolate, stats  # various convenience tools\nfrom skimage import transform  # image processing and registration\nimport subprocess\nimport sys\n\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\ntry:\n    # This package is not part of anaconda and may need to be installed.\n    import wget\nexcept ImportError:\n    warn('wget not found.  Will install with pip.')\n    import pip\n    install('wget')\n    import wget\ntry:\n    import pyUSID as usid # used mainly for visualization purposes here\nexcept ImportError:\n    warn('pyUSID not found.  Will install with pip.')\n    import pip\n    install('pyUSID')\n    import pyUSID as usid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining a few handy functions that will be reused multiple times\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def twin_image_plot(images, titles, cmap=plt.cm.viridis):\n    \"\"\"\n    Handy function that plots two images side by side with colorbars\n\n    Parameters\n    ----------\n    images : list or array-like\n        List of two images defined as 2D numpy arrays\n    titles : list or array-like\n        List of the titles for each image\n    cmap : (Optional) matplotlib.pyplot colormap object or string\n        Colormap to use for displaying the images\n\n    Returns\n    -------\n    fig : Figure\n        Figure containing the plots\n    axes : 1D array_like of axes objects\n        Axes of the individual plots within `fig`\n    \"\"\"\n    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n    for axis, img, title in zip(axes.flat, images, titles):\n        usid.plot_utils.plot_map(axis, img, cmap=cmap)\n        axis.set_title(title)\n    fig.tight_layout()\n    return fig, axes\n\n\ndef normalize_image(image):\n    \"\"\"\n    Normalizes the provided image from 0 to 1\n\n    Parameters\n    ----------\n    image : np.array object\n        Image to be normalized\n\n    Returns\n    -------\n    image : np.array object\n        Image normalized from 0 to 1\n    \"\"\"\n    return (image - np.amin(image)) / (np.amax(image) - np.amin(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data from the hdf5 file\n--------------------------------\nWe will be using an data file available on our GitHub project page by default. You are encouraged\nto download this document as a Jupyter Notebook (button at the bottom of the page) and use your own data instead.\nWhen using your own data, you can skip this cell and provide the path to your data using the variable - h5_path\n\nWe begin by loading the high resolution STM image, the Z component image of the spectroscopic data set, and the\nspectroscopic data set itself\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Downloading the example file from the pycroscopy Github project\nurl = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/sts_data_image_registration.h5'\nh5_path = 'temp.h5'\n_ = wget.download(url, h5_path, bar=None)\n\nprint('Working on:\\n' + h5_path)\n\nwith h5py.File(h5_path, mode='r') as h5_f:\n    sts_spectral_data = h5_f['sts_spectra'][()]  # STS spectral data set\n    high_res_topo = h5_f['stm_topography'][()]  # STM image\n    sts_z_contr = h5_f['sts_z_contrast'][()]  # STS Z contrast image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize images\n----------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "high_res_topo = normalize_image(high_res_topo)\nsts_z_contr = normalize_image(sts_z_contr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shapes of datasets\n------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('STS Spectra shape:', sts_spectral_data.shape)\nprint('STM Topography shape:', high_res_topo.shape)\nprint('STS Z contrast shape:', sts_z_contr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "visualization\n--------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = twin_image_plot([high_res_topo, sts_z_contr],\n                            ['STM topography', 'STS Z contrast'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpolate image and the Z channel data\n----------------------------------------\nSince our goal is to maximize overlap between the two datasets, we should create some additional datasets with\nmatching sizes. The Topo image data is 1024 x 1024 pixels, whereas the STS data is 40 x 40 spatial pixels (plus a 3rd\ndimension of 256 points)\n\nLet\u2019s create two additional images \u2013 an interpolated STS image that is now 1024 x 1024 and a reduced Topo image\nthat\u2019s 40 x 40\n\nFirst we create an interpolant model, and then evaluate it at an interval that would give us the desired image size\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_shape = sts_z_contr.shape\ntopo_shape = high_res_topo.shape\n\nz_upscaler = interpolate.RectBivariateSpline(np.arange(z_shape[0]),\n                                             np.arange(z_shape[1]),\n                                             sts_z_contr)\n\n# we now use the instance of the class to run it on a new set of values, which\n# are still 1 to 40, but now take 40/1024 size steps returning 1024x1024 image\nz_upscaled = z_upscaler(np.arange(0, z_shape[0], z_shape[0] / topo_shape[0]),\n                        np.arange(0, z_shape[1], z_shape[1] / topo_shape[1]))\n\n# repeat the process for downscaling as above, but simply reverse the direction\n# feed the fine steps first & then...\ntopo_downscaler = interpolate.RectBivariateSpline(np.arange(0, z_shape[0], z_shape[0] / topo_shape[0]),\n                                                  np.arange(0, z_shape[1], z_shape[1] / topo_shape[1]),\n                                                  high_res_topo)\n\n# use the class with only big steps, downscalign to a 40x40\ntopo_downscaled = topo_downscaler(np.arange(z_shape[0]),\n                                  np.arange(z_shape[1]))\n\n# visualization\nfig, axes = twin_image_plot([topo_downscaled, z_upscaled],\n                            ['Downscaled Topography to Z contrast size',\n                             'Z contrast upscaled to topography size'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing for image registration\n----------------------------------------\nWe now have a choice to make \u2013 whether we want to register two 40 x 40 images: Reduced Topo and the original STS, or\nregister the two 1024 x 1024 images: Original Topo and the interpolated STS, or do both.\n\nIf you have the time and resources, doing both is a good idea to get a handle on the quality of the registration.\nLet\u2019s try mapping the larger images in this example\n\nBefore getting started, it\u2019s a good idea to also think about what data you\u2019d like to use a reference and which data\nshould get transformed. Data that is to remain invariant is referred to as the \u2018fixed\u2019 image and the data to get the\ntransformation is referred to as \u2018moving.\u2019 In our case we want to map the STS data onto the image data, so Topo is\n\u2018fixed\u2019 and STS is \u2018moving\u2019\n\nPython does not automatically calculate a transformation matrix for you. This is only possible if your images contain\nsimilar features, which is the case here, as this quick registration is intensity based. There are ways to do this in\nPython using an OpenCV library. Since this library isn\u2019t installed natively with Anaconda, I invite you to explore\nthose concepts on your own.\nYou have to specify a transform matrix on your own, which is essentially a coordinate matching problem. This seems\nharder initially, but for most of the data this is the only way forward because features will not match in different\ninformation channels (topography & Raman for example)\n\nWe have selected few points quickly here. As the number of points is increased and the coordinates are more carefully\nselected, the registration quality improves substantially\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First normalize the up and downscaled images:\nz_upscaled = normalize_image(z_upscaled)\ntopo_downscaled = normalize_image(topo_downscaled)\n\n# define the topography as the image that is fixed and the upscaled Z contrast image as the one that moves\n# during the image registration\nfixed = high_res_topo\nmoving = z_upscaled\n\n# Define the points that are common:\nsrc = [(536, 482),\n       (100, 785),\n       (745, 294),\n       (604, 918)]\ndst = [(561, 527),\n       (193, 800),\n       (749, 332),\n       (678, 946)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the pointers on the two images\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First plot the two images\nfig, axes = twin_image_plot([high_res_topo, z_upscaled],\n                            ['Downscaled Topography to Z contrast size',\n                             'Z contrast upscaled to topography size'],\n                            cmap='gray')\n\n# Defining a quick function that plots markers on an image with different colors\ndef plot_markers(axis, coordinates, colors):\n    for clr, point in zip(colors, coordinates):\n        axis.scatter(point[0], point[1], color=clr, s=40)\n\n# Now add the markers\npointer_colors = ['b','y', 'g', 'r']\nplot_markers(axes[0], src, pointer_colors)\nplot_markers(axes[1], dst, pointer_colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its also a good idea to look at the overlaid raw data to gauge the difficulty of the transformation prior to starting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axis = plt.subplots(figsize=(5, 5))\naxis.imshow(fixed, cmap='Reds', alpha=0.8)\naxis.imshow(moving, cmap='Blues', alpha=0.8)\naxis.set_title('Images overlayed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image Registration\n----------------------------------------\nBefore starting the registration, we need to provide an optimizer configuration and a metric. OPTIMIZER contains\nsettings used to configure the intensity similarity optimization.  METRIC configures the image similarity metric to\nbe used during registration.\n\nThe next set of statements populates a transform_cell with different types of registrations available:\n\n* Translation \u2013 translation types of distortion\n* Rigid \u2013 translation and rotation types of distortion\n* Similarity \u2013 translation, rotation and scale types of distortion\n* Affine -- translation, rotation, scale and shear types of distortion\n\nIn cases when you aren\u2019t sure which transformation is best, and time is permitting. All 4 can be tried.\n\nLets take a closer look inside the transformation process:\n\nInside this loop we are registering an image, getting the transform, applying it to the \u2018moving\u2019 data set and\ninspecting the results.\nSince we are not sure which transform to pick, we are trying all of them one at a time \u2013 hence why this code is\ninside a \u2018for\u2019 loop\nWe use the Pearson Correlation to look at how well (higher number is better) each of the transforms performed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trans_names = ['similarity', 'affine', 'piecewise-affine', 'projective']  # transform list\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n\nfor tform_type, axis in zip(trans_names, axes.flat):  # looping through transforms\n\n    # build the model\n    tform = transform.estimate_transform(tform_type, np.array(src), np.array(dst))\n\n    # use the model\n    raw_corrected_Z = transform.warp(moving, inverse_map=tform.inverse, output_shape=np.shape(moving))\n\n    # one way to do correlations\n    corr = stats.pearsonr(np.reshape(fixed, [1024 * 1024, 1]),\n                          np.reshape(raw_corrected_Z, [1024 * 1024, 1]))[0][0]\n\n    # visualize the transformation\n    axis.set_title(tform_type + ' - Pearson corr: ' + str(np.round(corr, 3)))\n    axis.imshow(raw_corrected_Z)\n\nfig.suptitle('Different transforms applied to the images', y=1.03)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "delete the h5_file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os.remove(h5_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}