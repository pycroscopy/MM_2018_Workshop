{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n================================================================================\n05. Utilities for reading h5USID files\n================================================================================\n\n**Suhas Somnath**\n\n4/18/2018\n\n**This document illustrates the many handy functions in pyUSID.hdf_utils that significantly simplify reading data\nand metadata in Universal Spectroscopy and Imaging Data (USID) HDF5 files (h5USID files)**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n-------------\nThe USID model uses a data-centric approach to data analysis and processing meaning that results from all data analysis\nand processing are written to the same h5 file that contains the recorded measurements. **Hierarchical Data Format\n(HDF5)** files allow data, whether it is raw measured data or results of analysis, to be stored in multiple datasets within\nthe same file in a tree-like manner. Certain rules and considerations have been made in pyUSID to ensure\nconsistent and easy access to any data.\n\nThe h5py python package provides great functions to create, read, and manage data in HDF5 files. In\n``pyUSID.hdf_utils``, we have added functions that facilitate scientifically relevant, or USID specific\nfunctionality such as checking if a dataset is a Main dataset, reshaping to / from the original N dimensional form of\nthe data, etc. Due to the wide breadth of the functions in ``hdf_utils``, the guide for hdf_utils will be split in two\nparts - one that focuses on functions that facilitate reading and one that facilitate writing of data. The following\nguide provides examples of how, and more importantly when, to use functions in ``pyUSID.hdf_utils`` for various\nscenarios.\n\nRecommended pre-requisite reading\n-----------------------------------\n* `USID data model </../../data_format.html>`_\n* `Crash course on HDF5 and h5py <./plot_h5py.html>`_\n* Utilities for `writing <./plot_hdf_utils_write.html>`_ h5USID files using pyUSID\n\nImport all necessary packages\n-------------------------------\n\nBefore we begin demonstrating the numerous functions in ``pyUSID.hdf_utils``, we need to import the necessary\npackages. Here are a list of packages besides pyUSID that will be used in this example:\n\n* ``h5py`` - to open and close the file\n* ``wget`` - to download the example data file\n* ``numpy`` - for numerical operations on arrays in memory\n* ``matplotlib`` - basic visualization of data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division, unicode_literals\nimport os\n# Warning package in case something goes wrong\nfrom warnings import warn\nimport subprocess\nimport sys\n\n\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n# Package for downloading online files:\n\ntry:\n    # This package is not part of anaconda and may need to be installed.\n    import wget\nexcept ImportError:\n    warn('wget not found.  Will install with pip.')\n    import pip\n    install(wget)\n    import wget\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Finally import pyUSID.\ntry:\n    import pyUSID as usid\nexcept ImportError:\n    warn('pyUSID not found.  Will install with pip.')\n    import pip\n    install('pyUSID')\n    import pyUSID as usid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to demonstrate the many functions in hdf_utils, we will be using a h5USID file containing real\nexperimental data along with results from analyses on the measurement data\n\nThis scientific dataset\n-----------------------\n\nFor this example, we will be working with a **Band Excitation Polarization Switching (BEPS)** dataset acquired from\nadvanced atomic force microscopes. In the much simpler **Band Excitation (BE)** imaging datasets, a single spectrum is\nacquired at each location in a two dimensional grid of spatial locations. Thus, BE imaging datasets have two\nposition dimensions (``X``, ``Y``) and one spectroscopic dimension (``Frequency`` - against which the spectrum is recorded).\nThe BEPS dataset used in this example has a spectrum for **each combination of** three other parameters (``DC offset``,\n``Field``, and ``Cycle``). Thus, this dataset has three new spectral dimensions in addition to ``Frequency``. Hence,\nthis dataset becomes a 2+4 = **6 dimensional dataset**\n\nLoad the dataset\n------------------\nFirst, let us download this file from the pyUSID Github project:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/pycroscopy/pyUSID/master/data/BEPS_small.h5'\nh5_path = 'temp.h5'\n_ = wget.download(url, h5_path, bar=None)\n\nprint('Working on:\\n' + h5_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, lets open this HDF5 file in read-only mode. Note that opening the file does not cause the contents to be\nautomatically loaded to memory. Instead, we are presented with objects that refer to specific HDF5 datasets,\nattributes or groups in the file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_path = 'temp.h5'\nh5_f = h5py.File(h5_path, mode='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, ``h5_f`` is an active handle to the open file\n\nInspect HDF5 contents\n======================\n\nThe file contents are stored in a tree structure, just like files on a contemporary computer. The file contains\ngroups (similar to file folders) and datasets (similar to spreadsheets).\nThere are several datasets in the file and these store:\n\n* The actual measurement collected from the experiment\n* Spatial location on the sample where each measurement was collected\n* Information to support and explain the spectral data collected at each location\n* Since the USID model stores results from processing and analyses performed on the data in the same h5USID file,\n  these datasets and groups are present as well\n* Any other relevant ancillary information\n\nprint_tree()\n------------\nSoon after opening any file, it is often of interest to list the contents of the file. While one can use the open\nsource software HDFViewer developed by the HDF organization, ``pyUSID.hdf_utils`` also has a very handy function -\n``print_tree()`` to quickly visualize all the datasets and groups within the file within python.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Contents of the H5 file:')\nusid.hdf_utils.print_tree(h5_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, ``print_tree()`` presents a clean tree view of the contents of the group. In this mode, only the group names\nare underlined. Alternatively, it can print the full paths of each dataset and group, with respect to the group / file\nof interest, by setting the ``rel_paths``\nkeyword argument. ``print_tree()`` could also be used to display the contents of and HDF5 group instead of complete HDF5\nfile as we have done above. Lets configure it to print the relative paths of all objects within the ``Channel_000``\ngroup:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "usid.hdf_utils.print_tree(h5_f['/Measurement_000/Channel_000/'], rel_paths=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, ``print_tree()`` can also be configured to only print USID Main datasets besides Group objects using the\n``main_dsets_only`` option\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "usid.hdf_utils.print_tree(h5_f, main_dsets_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing Attributes\n==================================\n\nHDF5 datasets and groups can also store metadata such as experimental parameters. These metadata can be text,\nnumbers, small lists of numbers or text etc. These metadata can be very important for understanding the datasets\nand guide the analysis routines.\n\nWhile one could use the basic ``h5py`` functionality to access attributes, one would encounter a lot of problems when\nattempting to decode attributes whose values were strings or lists of strings due to some issues in ``h5py``. This problem\nhas been demonstrated in our `primer to HDF5 and h5py <./plot_h5py.html>`_. Instead of using the basic functionality of ``h5py``, we recommend always\nusing the functions in pyUSID that reliably and consistently work for any kind of attribute for any version of\npython:\n\nget_attributes()\n----------------\n\n``get_attributes()`` is a very handy function that returns all or a specified set of attributes in an HDF5 object. If no\nattributes are explicitly requested, all attributes in the object are returned:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for key, val in usid.hdf_utils.get_attributes(h5_f).items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``get_attributes()`` is also great for only getting selected attributes. For example, if we only cared about the user\nand project related attributes, we could manually request for any that we wanted:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proj_attrs = usid.hdf_utils.get_attributes(h5_f, ['project_name', 'project_id', 'user_name'])\nfor key, val in proj_attrs.items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get_attr()\n----------\n\nIf we are sure that we only wanted a specific attribute, we could instead use ``get_attr()`` as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(usid.hdf_utils.get_attr(h5_f, 'user_name'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "check_for_matching_attrs()\n--------------------------\nConsider the scenario where we are have several HDF5 files or Groups or datasets and we wanted to check each one to\nsee if they have the certain metadata / attributes. ``check_for_matching_attrs()`` is one very handy function that\nsimplifies the comparision operation.\n\nFor example, let us check if this file was authored by ``John Doe``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(usid.hdf_utils.check_for_matching_attrs(h5_f, new_parms={'user_name': 'John Doe'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding datasets and groups\n============================\n\nThere are numerous ways to search for and access datasets and groups in H5 files using the basic functionalities\nof h5py. pyUSID.hdf_utils contains several functions that simplify common searching / lookup operations as part of\nscientific workflows.\n\nfind_dataset()\n----------------\n\nThe ``find_dataset()`` function will return all datasets that whose names contain the provided string. In this case, we\nare looking for any datasets containing the string ``UDVS`` in their names. If you look above, there are two datasets\n(UDVS and UDVS_Indices) that match this condition:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "udvs_dsets_2 = usid.hdf_utils.find_dataset(h5_f, 'UDVS')\nfor item in udvs_dsets_2:\n    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you might know by now, h5USID files contain three kinds of datasets:\n\n* ``Main`` datasets that contain data recorded / computed at multiple spatial locations.\n* ``Ancillary`` datasets that support a main dataset\n* Other datasets\n\nFor more information, please refer to the documentation on the USID model.\n\ncheck_if_main()\n---------------\n``check_if_main()`` is a very handy function that helps distinguish between ``Main`` datasets and other objects\n(``Ancillary`` datasets, other datasets, Groups etc.). Lets apply this function to see which of the objects within the\n``Channel_000`` Group are ``Main`` datasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_chan_group = h5_f['Measurement_000/Channel_000']\n\n# We will prepare two lists - one of objects that are ``main`` and one of objects that are not\n\nnon_main_objs = []\nmain_objs = []\nfor key, val in h5_chan_group.items():\n    if usid.hdf_utils.check_if_main(val):\n        main_objs.append(key)\n    else:\n        non_main_objs.append(key)\n\n# Now we simply print the names of the items in each list\n\nprint('Main Datasets:')\nprint('----------------')\nfor item in main_objs:\n    print(item)\nprint('\\nObjects that were not Main datasets:')\nprint('--------------------------------------')\nfor item in non_main_objs:\n    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above script allowed us to distinguish Main datasets from all other objects only within the Group named\n``Channel_000``.\n\nget_all_main()\n--------------\nWhat if we want to quickly find all ``Main`` datasets even within the sub-Groups of ``Channel_000``? To do this, we have a\nvery handy function called - ``get_all_main()``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_dsets = usid.hdf_utils.get_all_main(h5_chan_group)\nfor dset in main_dsets:\n    print(dset)\n    print('--------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The datasets above show that the file contains three main datasets. Two of these datasets are contained in a HDF5\nGroup called ``Raw_Data-SHO_Fit_000`` meaning that they are results of an operation called ``SHO_Fit`` performed on the\n``Main`` dataset - ``Raw_Data``. The first of the three main datasets is indeed the ``Raw_Data`` dataset from which the\nlatter two datasets (``Fit`` and ``Guess``) were derived.\n\nThe USID model allows the same operation, such as ``SHO_Fit``, to be performed on the same dataset (``Raw_Data``),\nmultiple\ntimes. Each time the operation is performed, a new HDF5 Group is created to hold the new results. Often, we may\nwant to perform a few operations such as:\n\n* Find the (source / main) dataset from which certain results were derived\n* Check if a particular operation was performed on a main dataset\n* Find all groups corresponding to a particular operation (e.g. - ``SHO_Fit``) being applied to a Main dataset\n\n``hdf_utils`` has a few handy functions for many of these use cases.\n\nfind_results_groups()\n----------------------\nFirst, lets show that ``find_results_groups()`` finds all Groups containing the results of a ``SHO_Fit`` operation applied\nto ``Raw_Data``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First get the dataset corresponding to Raw_Data\nh5_raw = h5_chan_group['Raw_Data']\n\noperation = 'SHO_Fit'\nprint('Instances of operation \"{}\" applied to dataset named \"{}\":'.format(operation, h5_raw.name))\nh5_sho_group_list = usid.hdf_utils.find_results_groups(h5_raw, operation)\nprint(h5_sho_group_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the ``SHO_Fit`` operation was performed on ``Raw_Data`` dataset only once, which is why\n``find_results_groups()`` returned only one HDF5 Group - ``SHO_Fit_000``.\n\ncheck_for_old()\n-----------------\n\nOften one may want to check if a certain operation was performed on a dataset with the very same parameters to\navoid recomputing the results. ``hdf_utils.check_for_old()`` is a very handy function that compares parameters (a\ndictionary) for a new / potential operation against the metadata (attributes) stored in each existing results group\n(HDF5 groups whose name starts with ``Raw_Data-SHO_Fit`` in this case). Before we demonstrate ``check_for_old()``, lets\ntake a look at the attributes stored in the existing results groups:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Parameters already used for computing SHO_Fit on Raw_Data in the file:')\nfor key, val in usid.hdf_utils.get_attributes(h5_chan_group['Raw_Data-SHO_Fit_000']).items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us check for existing results where the ``SHO_fit_method`` attribute matches an existing value and a new value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Checking to see if SHO Fits have been computed on the raw dataset:')\nprint('\\nUsing \"pycroscopy BESHO\":')\nprint(usid.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n                                 new_parms={'SHO_fit_method': 'pycroscopy BESHO'}))\nprint('\\nUsing \"alternate technique\"')\nprint(usid.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n                                 new_parms={'SHO_fit_method': 'alternate technique'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, while find_results_groups() returned any and all groups corresponding to ``SHO_Fit`` being applied to\n``Raw_Data``, ``check_for_old()`` only returned the group(s) where the operation was performed using the same specified\nparameters (``sho_fit_method`` in this case).\n\nNote that ``check_for_old()`` performs two operations - search for all groups with the matching nomenclature and then\ncompare the attributes. ``check_for_matching_attrs()`` is the handy function, that enables the latter operation of\ncomparing a giving dictionary of parameters against attributes in a given object.\n\nget_source_dataset()\n---------------------\n``hdf_utils.get_source_dataset()`` is a very handy function for the inverse scenario where we are interested in finding\nthe source dataset from which the known result was derived:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_sho_group = h5_sho_group_list[0]\nprint('Datagroup containing the SHO fits:')\nprint(h5_sho_group)\nprint('\\nDataset on which the SHO Fit was computed:')\nh5_source_dset = usid.hdf_utils.get_source_dataset(h5_sho_group)\nprint(h5_source_dset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the source dataset is always a ``Main`` dataset, ``get_source_dataset()`` results a ``USIDataset`` object instead of\na regular ``HDF5 Dataset`` object.\n\nNote that ``hdf_utils.get_source_dataset()`` and ``find_results_groups()`` rely on the USID rule that results of an\noperation be stored in a Group named ``Source_Dataset_Name-Operation_Name_00x``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# get_auxiliary_datasets()\n# -------------------------\n# The association of datasets and groups with one another provides a powerful mechanism for conveying (richer)\n# information. One way to associate objects with each other is to store the reference of an object as an attribute of\n# another. This is precisely the capability that is leveraged to turn Central datasets into USID Main Datasets or\n# ``USIDatasets``. USIDatasets need to have four attributes that are references to the ``Position`` and ``Spectroscopic``\n# ``ancillary`` datasets. Note, that USID does not restrict or preclude the storage of other relevant datasets as\n# attributes of another dataset.\n#\n# For example, the ``Raw_Data`` dataset appears to contain several attributes whose keys / names match the names of\n# datasets we see above and values all appear to be HDF5 object references:\n\nfor key, val in usid.hdf_utils.get_attributes(h5_raw).items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the name suggests, these HDF5 object references are references or addresses to datasets located elsewhere in the\nfile. Conventionally, one would need to apply this reference to the file handle to get the actual HDF5 Dataset / Group\nobject.\n\n``get_auxiliary_datasets()`` simplifies this process by directly retrieving the actual Dataset / Group associated with\nthe attribute. Thus, we would be able to get a reference to the ``Bin_Frequencies`` Dataset via:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_obj = usid.hdf_utils.get_auxiliary_datasets(h5_raw, 'Bin_Frequencies')[0]\nprint(h5_obj)\n# Lets prove that this object is the same as the 'Bin_Frequencies' object that can be directly addressed:\nprint(h5_obj == h5_f['/Measurement_000/Channel_000/Bin_Frequencies'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing Ancillary Datasets\n=============================\nOne of the major benefits of h5USID is its ability to handle large multidimensional datasets at ease. ``Ancillary``\ndatasets serve as the keys or legends for explaining the dimensionality, reshape-ability, etc. of a dataset. There are\nseveral functions in hdf_utils that simplify many common operations on ancillary datasets.\n\nBefore we demonstrate the several useful functions in hdf_utils, lets access the position and spectroscopic ancillary\ndatasets using the ``get_auxiliary_datasets()`` function we used above:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dset_list = usid.hdf_utils.get_auxiliary_datasets(h5_raw, ['Position_Indices', 'Position_Values',\n                                                         'Spectroscopic_Indices', 'Spectroscopic_Values'])\nh5_pos_inds, h5_pos_vals, h5_spec_inds, h5_spec_vals = dset_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, this is indeed a six dimensional dataset with two position dimensions and four spectroscopic\ndimensions. The ``Field`` and ``Cycle`` dimensions do not have any units since they are dimensionless unlike the other\ndimensions.\n\nget_dimensionality()\n---------------------\nNow lets find out the number of steps in each of those dimensions using another handy function called\n``get_dimensionality()``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pos_dim_sizes = usid.hdf_utils.get_dimensionality(h5_pos_inds)\nspec_dim_sizes = usid.hdf_utils.get_dimensionality(h5_spec_inds)\npos_dim_names = usid.hdf_utils.get_attr(h5_pos_inds, 'labels')\nspec_dim_names = usid.hdf_utils.get_attr(h5_spec_inds, 'labels')\n\nprint('Size of each Position dimension:')\nfor name, length in zip(pos_dim_names, pos_dim_sizes):\n    print('{} : {}'.format(name, length))\nprint('\\nSize of each Spectroscopic dimension:')\nfor name, length in zip(spec_dim_names, spec_dim_sizes):\n    print('{} : {}'.format(name, length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get_sort_order()\n----------------\n\nIn a few (rare) cases, the spectroscopic / position dimensions are not arranged in descending order of rate of change.\nIn other words, the dimensions in these ancillary matrices are not arranged from fastest-varying to slowest.\nTo account for such discrepancies, ``hdf_utils`` has a very handy function that goes through each of the columns or\nrows in the ancillary indices matrices and finds the order in which these dimensions vary.\n\nBelow we illustrate an example of sorting the names of the spectroscopic dimensions from fastest to slowest in\nthe BEPS data file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spec_sort_order = usid.hdf_utils.get_sort_order(h5_spec_inds)\nprint('Rate of change of spectroscopic dimensions: {}'.format(spec_sort_order))\nprint('\\nSpectroscopic dimensions arranged as is:')\nprint(spec_dim_names)\nsorted_spec_labels = np.array(spec_dim_names)[np.array(spec_sort_order)]\nprint('\\nSpectroscopic dimensions arranged from fastest to slowest')\nprint(sorted_spec_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get_unit_values()\n-----------------\n\nWhen visualizing the data it is essential to plot the data against appropriate values on the X, Y, or Z axes.\nRecall that by definition that the values over which each dimension is varied, are repeated and tiled over the entire\nposition or spectroscopic dimension of the dataset. Thus, if we had just the bias waveform repeated over two cycles,\nspectroscopic values would contain the bias waveform tiled twice and the cycle numbers repeated as many times as the\nnumber of points in the bias waveform. Therefore, extracting the bias waveform or the cycle numbers from the ancillary\ndatasets is not trivial. This problem is especially challenging for multidimensional datasets such as the one under\nconsideration. Fortunately, ``hdf_utils`` has a very handy function for this as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pos_unit_values = usid.hdf_utils.get_unit_values(h5_pos_inds, h5_pos_vals)\nprint('Position unit values:')\nfor key, val in pos_unit_values.items():\n    print('{} : {}'.format(key, val))\nspec_unit_values = usid.hdf_utils.get_unit_values(h5_spec_inds, h5_spec_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the spectroscopic dimensions are quite complicated, lets visualize the results from ``get_unit_values()``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(6.5, 6))\nfor axis, name in zip(axes.flat, spec_dim_names):\n    axis.set_title(name)\n    axis.plot(spec_unit_values[name], 'o-')\n\nfig.suptitle('Spectroscopic Dimensions', fontsize=16, y=1.05)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reshaping Data\n==============\n\nreshape_to_n_dims()\n-------------------\n\nThe USID model stores N dimensional datasets in a flattened 2D form of position x spectral values. It can become\nchallenging to retrieve the data in its original N-dimensional form, especially for multidimensional datasets such as\nthe one we are working on. Fortunately, all the information regarding the dimensionality of the dataset are contained\nin the spectral and position ancillary datasets. ``reshape_to_n_dims()`` is a very useful function that can help\nretrieve the N-dimensional form of the data using a simple function call:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ndim_form, success, labels = usid.hdf_utils.reshape_to_n_dims(h5_raw, get_labels=True)\nif success:\n    print('Succeeded in reshaping flattened 2D dataset to N dimensions')\n    print('Shape of the data in its original 2D form')\n    print(h5_raw.shape)\n    print('Shape of the N dimensional form of the dataset:')\n    print(ndim_form.shape)\n    print('And these are the dimensions')\n    print(labels)\nelse:\n    print('Failed in reshaping the dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reshape_from_n_dims()\n-----------------------\nThe inverse problem of reshaping an N dimensional dataset back to a 2D dataset (let's say for the purposes of\nmultivariate analysis or storing into h5USID files) is also easily solved using another handy\nfunction - ``reshape_from_n_dims()``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "two_dim_form, success = usid.hdf_utils.reshape_from_n_dims(ndim_form, h5_pos=h5_pos_inds, h5_spec=h5_spec_inds)\nif success:\n    print('Shape of flattened two dimensional form')\n    print(two_dim_form.shape)\nelse:\n    print('Failed in flattening the N dimensional dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Close and delete the h5_file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_f.close()\nos.remove(h5_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}