{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n================================================================================\n06. Utilities for handling data types and transformations\n================================================================================\n\n**Suhas Somnath**\n\n4/18/2018\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n-------------\nThe general nature of the **Universal Spectroscopy and Imaging Data (USID)** model facilitates the representation of\nany kind of measurement data.\nThis includes:\n\n #. Conventional data represented using floating point numbers such as ``1.2345``\n #. Integer data (with or without sign) such as ``137``\n #. Complex-valued data such as ``1.23 + 4.5i``\n #. Multi-valued or compound valued data cells such as (``'Frequency'``: ``301.2``, ``'Amplitude'``: ``1.553E-3``, ``'Phase'``: ``2.14``)\n    where a single value or measurement is represented by multiple elements, each with their own names, and data types\n\nWhile HDF5 datasets are capable of storing all of these kinds of data, many conventional data analysis techniques\nsuch as decomposition, clustering, etc. are either unable to handle complicated data types such as complex-valued\ndatasets and compound valued datasets, or the results from these techniques do not produce physically meaningful\nresults. For example, most singular value decomposition algorithms are capable of processing complex-valued datasets.\nHowever, while the eigenvectors can have complex values, the resultant complex-valued abundance maps are meaningless.\nThese algorithms would not even work if the original data was compound valued!\n\nTo avoid such problems, we need functions that transform the data to and from the necessary type (integer, real-value\netc.)\n\nThe ``pyUSID.dtype_utils`` module facilitates comparisons, validations, and most importantly, transformations of one\ndata-type to another. We will be going over the many useful functions in this module and explaining how, when and why\none would use them.\n\nRecommended pre-requisite reading\n-----------------------------------\n* `USID data model </../../data_format.html>`_\n* `Crash course on HDF5 and h5py <./plot_h5py.html>`_\n\nImport all necessary packages\n-------------------------------\nBefore we begin demonstrating the numerous functions in ``pyUSID.dtype_utils``, we need to import the necessary\npackages. Here are a list of packages besides pyUSID that will be used in this example:\n\n* ``h5py`` - to manipulate HDF5 files\n* ``numpy`` - for numerical operations on arrays in memory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division, unicode_literals\nimport os\nimport subprocess\nimport sys\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nimport h5py\nimport numpy as np\n# Finally import pyUSID.\ntry:\n    import pyUSID as usid\nexcept ImportError:\n    # Warning package in case something goes wrong\n    from warnings import warn\n    warn('pyUSID not found.  Will install with pip.')\n    import pip\n    install('pyUSID')\n    import pyUSID as usid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilities for validating data types\n=====================================\npyUSID.dtype_utils contains some handy functions that make it easy to write robust and safe code by simplifying\ncommon data type checking and validation.\n\ncontains_integers()\n---------------------\nThe ``contains_integers()`` function checks to make sure that each item in a list is indeed an integer. Additionally, it\ncan be configured to ensure that all the values are above a minimum value. This is particularly useful when building\nindices matrices based on the size of dimensions - specified as a list of integers for example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "item = [1, 2, -3, 4]\nprint('{} : contains integers? : {}'.format(item, usid.dtype_utils.contains_integers(item)))\nitem = [1, 4.5, 2.2, -1]\nprint('{} : contains integers? : {}'.format(item, usid.dtype_utils.contains_integers(item)))\n\nitem = [1, 5, 8, 3]\nmin_val = 2\nprint('{} : contains integers >= {} ? : {}'.format(item, min_val,\n                                                usid.dtype_utils.contains_integers(item, min_val=min_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "validate_dtype()\n-----------------\nThe ``validate_dtype()`` function ensure that a provided object is indeed a valid h5py or numpy data type. When writing\na main dataset along with all ancillary datasets, pyUSID meticulously ensures that all inputs are valid before\nwriting data to the file. This comes in very handy when we want to follow the 'measure twice, cut once' ethos.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for item in [np.float16, np.complex64, np.uint8, np.int16]:\n    print('Is {} a valid dtype? : {}'.format(item, usid.dtype_utils.validate_dtype(item)))\n\n\n# This function is especially useful on compound or structured data types:\n\nstruct_dtype = np.dtype({'names': ['r', 'g', 'b'],\n                        'formats': [np.float32, np.uint16, np.float64]})\nprint('Is {} a valid dtype? : {}'.format(struct_dtype, usid.dtype_utils.validate_dtype(struct_dtype)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get_compound_sub_dtypes()\n--------------------------\nOne common hassle when dealing with compound / structured array dtypes is that it can be a little challenging to\nquickly get the individual datatypes of each field in such a data type. The ``get_compound_sub_dtypes()`` makes this a\nlot easier:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sub_dtypes = usid.dtype_utils.get_compound_sub_dtypes(struct_dtype)\nfor key, val in sub_dtypes.items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "is_complex_dtype()\n-------------------\nQuite often, we need to treat complex datasets different from compound datasets which themselves need to be treated\ndifferent from real valued datasets. ``is_complex_dtype()`` makes it easier to check if a numpy or HDF5 dataset has a\ncomplex data type:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for dtype in [np.float32, np.float16, np.uint8, np.int16, struct_dtype, bool]:\n    print('Is {} a complex dtype?: {}'.format(dtype, (usid.dtype_utils.is_complex_dtype(dtype))))\n\nfor dtype in [np.complex, np.complex64, np.complex128, np.complex256]:\n    print('Is {} a complex dtype?: {}'.format(dtype, (usid.dtype_utils.is_complex_dtype(dtype))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data transformation\n====================\nPerhaps the biggest benefit of ``dtype_utils`` is the ability to flatten complex, compound datasets to real-valued\ndatasets and vice versa. As mentioned in the introduction, this is particularly important when attempting to use\nmachine learning algorithms on complex or compound-valued datasets. In order to enable such pipelines, we need\nfunctions to transform:\n\n* complex / compound valued datasets to real-valued datasets\n* real-valued datasets back to complex / compound valued datasets\n\nflatten_complex_to_real()\n--------------------------\nAs the name suggests, this function stacks the imaginary values of a N-dimensional numpy / HDF5 dataset below its\nreal-values. Thus, applying this function to a complex valued dataset of size ``(a, b, c)`` would result in a\nreal-valued dataset of shape ``(a, b, 2 * c)``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "length = 3\ncomplex_array = np.random.randint(-5, high=5, size=length) + 1j * np.random.randint(-5, high=5, size=length)\nstacked_real_array = usid.dtype_utils.flatten_complex_to_real(complex_array)\nprint('Complex value: {} has shape: {}'.format(complex_array, complex_array.shape))\nprint('Stacked real value: {} has shape: '\n      '{}'.format(stacked_real_array, stacked_real_array.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "flatten_compound_to_real()\n----------------------------\nThis function flattens a compound-valued dataset of shape ``(a, b, c)`` into a real-valued dataset of shape\n``(a, b, k * c)`` where ``k`` is the number of fields within the structured array / compound dtype. Here we will\ndemonstrate this on a 1D array of 5 elements each containing 'r', 'g', 'b' fields:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_elems = 5\nstructured_array = np.zeros(shape=num_elems, dtype=struct_dtype)\nstructured_array['r'] = np.random.random(size=num_elems) * 1024\nstructured_array['g'] = np.random.randint(0, high=1024, size=num_elems)\nstructured_array['b'] = np.random.random(size=num_elems) * 1024\nreal_array = usid.dtype_utils.flatten_compound_to_real(structured_array)\n\nprint('Structured array is of shape {} and have values:'.format(structured_array.shape))\nprint(structured_array)\nprint('\\nThis array converted to regular scalar matrix has shape: {} and values:'.format(real_array.shape))\nprint(real_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "flatten_to_real()\n-----------------\nThis function checks the data type of the provided dataset and then uses either of the above functions to\n(if necessary) flatten the dataset into a real-valued matrix. By checking the data type of the dataset, it obviates\nthe need to explicitly call the aforementioned functions (that still do the work). Here is an example of the function\nbeing applied to the compound valued numpy array again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "real_array = usid.dtype_utils.flatten_to_real(structured_array)\nprint('Structured array is of shape {} and have values:'.format(structured_array.shape))\nprint(structured_array)\nprint('\\nThis array converted to regular scalar matrix has shape: {} and values:'.format(real_array.shape))\nprint(real_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next three functions perform the inverse operation of taking real-valued matrices or datasets and converting them\nto complex or compound-valued datasets.\n\nstack_real_to_complex()\n------------------------\nAs the name suggests, this function collapses a N dimensional real-valued array of size ``(a, b, 2 * c)`` to a\ncomplex-valued array of shape ``(a, b, c)``. It assumes that the first c values in real-valued dataset are the real\ncomponents and the following c values are the imaginary components of the complex value. This will become clearer\nwith an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "real_val = np.hstack([5 * np.random.rand(6),\n                      7 * np.random.rand(6)])\nprint('Real valued dataset of shape {}:'.format(real_val.shape))\nprint(real_val)\n\ncomp_val = usid.dtype_utils.stack_real_to_complex(real_val)\n\nprint('\\nComplex-valued array of shape: {}'.format(comp_val.shape))\nprint(comp_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "stack_real_to_compound()\n--------------------------\nSimilar to the above function, this function shrinks the last axis of a real valued dataset to create the desired\ncompound valued dataset. Here we will demonstrate it on the same 3-field ``(r,g,b)`` compound datatype:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_elems = 5\nreal_val = np.concatenate((np.random.random(size=num_elems) * 1024,\n                           np.random.randint(0, high=1024, size=num_elems),\n                           np.random.random(size=num_elems) * 1024))\nprint('Real valued dataset of shape {}:'.format(real_val.shape))\nprint(real_val)\n\ncomp_val = usid.dtype_utils.stack_real_to_compound(real_val, struct_dtype)\n\nprint('\\nStructured array of shape: {}'.format(comp_val.shape))\nprint(comp_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "stack_real_to_target_dtype()\n-----------------------------\nThis function performs the inverse of ``flatten_to_real()`` - stacks the provided real-valued dataset into a complex or\ncompound valued dataset using the two above functions. Note that unlike ``flatten_to_real()``, the target data type must\nbe supplied to the function for this to work:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Real valued dataset of shape {}:'.format(real_val.shape))\nprint(real_val)\n\ncomp_val = usid.dtype_utils.stack_real_to_target_dtype(real_val, struct_dtype)\n\nprint('\\nStructured array of shape: {}'.format(comp_val.shape))\nprint(comp_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "check_dtype()\n--------------\n``check_dtype()`` is a master function that figures out the data type, necessary function to transform a HDF5 dataset to\na real-valued array, expected data shape, etc. Before we demonstrate this function, we need to quickly create an\nexample HDF5 dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file_path = 'dtype_utils_example.h5'\nif os.path.exists(file_path):\n    os.remove(file_path)\nwith h5py.File(file_path) as h5_f:\n    num_elems = (5, 7)\n    structured_array = np.zeros(shape=num_elems, dtype=struct_dtype)\n    structured_array['r'] = 450 * np.random.random(size=num_elems)\n    structured_array['g'] = np.random.randint(0, high=1024, size=num_elems)\n    structured_array['b'] = 3178 * np.random.random(size=num_elems)\n    _ = h5_f.create_dataset('compound', data=structured_array)\n    _ = h5_f.create_dataset('real', data=450 * np.random.random(size=num_elems), dtype=np.float16)\n    _ = h5_f.create_dataset('complex', data=np.random.random(size=num_elems) + 1j * np.random.random(size=num_elems),\n                            dtype=np.complex64)\n    h5_f.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets test the the function on compound-, complex-, and real-valued HDF5 datasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def check_dataset(h5_dset):\n    print('\\tDataset being tested: {}'.format(h5_dset))\n    func, is_complex, is_compound, n_features, type_mult = usid.dtype_utils.check_dtype(h5_dset)\n    print('\\tFunction to transform to real: %s' % func)\n    print('\\tis_complex? %s' % is_complex)\n    print('\\tis_compound? %s' % is_compound)\n    print('\\tShape of dataset in its current form: {}'.format(h5_dset.shape))\n    print('\\tAfter flattening to real, shape is expected to be: ({}, {})'.format(h5_dset.shape[0], n_features))\n    print('\\tByte-size of a single element in its current form: {}'.format(type_mult))\n\n\nwith h5py.File(file_path, mode='r') as h5_f:\n    print('Checking a compound-valued dataset:')\n    check_dataset(h5_f['compound'])\n    print('')\n    print('Checking a complex-valued dataset:')\n    check_dataset(h5_f['complex'])\n    print('')\n    print('Checking a real-valued dataset:')\n    check_dataset(h5_f['real'])\nos.remove(file_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}