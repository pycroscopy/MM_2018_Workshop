{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n================================================================================\n08. Utilities that assist in writing USID data\n================================================================================\n\n**Suhas Somnath**\n\n4/18/2018\n\n**This document illustrates certain helper functions that simplify writing data to Universal Spectroscopy and Imaging\nData (USID) HDF5 files or h5USID files**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n-------------\nThe USID model takes a truly unique approach towards towards storing scientific observational data. Several helper\nfunctions are necessary to simplify the actual file writing process. ``pyUSID.write_utils`` is the home for those\nutilities that assist in writing data but do not directly interact with HDF5 files (as in ``pyUSID.hdf_utils``).\n``pyUSID.write_utils`` consist mainly of two broad categories of functions:\n* functions that assist in building ancillary datasets\n* miscellaneous functions that assist in formatting data\n\n**Note that most of these are low-level functions that are used by popular high level functions in\npyUSID.hdf_utils to simplify the writing of datasets.**\n\nRecommended pre-requisite reading\n----------------------------------\n* `USID model </../../data_format.html>`_\n\nWhat to read after this\n-------------------------\n* `Crash course on HDF5 and h5py <./plot_h5py.html>`_\n* Utilities for `reading <./plot_hdf_utils_read.html>`_ and `writing <./plot_hdf_utils_write.html>`_ h5USID files\n\nImport necessary packages\n--------------------------\nWe only need a handful of packages besides pyUSID to illustrate the functions in ``pyUSID.write_utils``:\n\n* ``numpy`` - for numerical operations on arrays in memory\n* ``matplotlib`` - basic visualization of data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division, unicode_literals\n# Warning package in case something goes wrong\nfrom warnings import warn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport subprocess\nimport sys\n\n\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n# Package for downloading online files:\n# Finally import pyUSID.\ntry:\n    import pyUSID as usid\nexcept ImportError:\n    warn('pyUSID not found.  Will install with pip.')\n    import pip\n    install('pyUSID')\n    import pyUSID as usid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building Ancillary datasets\n============================\nThe USID model uses pairs of ``ancillary matrices`` to support the instrument-agnostic and compact representation of\nmultidimensional datasets. While the creation of ``ancillary datasets`` is straightforward when the number of position\nand spectroscopic dimensions are relatively small (0-2), one needs to be careful when building these\n``ancillary datasets`` for datasets with large number of position / spectroscopic dimensions (> 2). The main challenge\ninvolves the careful tiling and repetition of unit vectors for each dimension with respect to the sizes of all other\ndimensions. Fortunately, ``pyUSID.write_utils`` has many handy functions that solve this problem.\n\nIn order to demonstrate the functions, lets say that we are working on an example ``Main dataset`` that has three\nspectroscopic dimensions (``Bias``, ``Field``, ``Cycle``). The Bias dimension varies as a bi-polar triangular waveform. This\nwaveform is repeated for two Fields over 3 Cycles meaning that the ``Field`` and ``Cycle`` dimensions are far simpler than\nthe ``Bias`` dimension in that they have linearly increasing / decreasing values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_v = 4\nhalf_pts = 8\nbi_triang = np.roll(np.hstack((np.linspace(-max_v, max_v, half_pts, endpoint=False),\n                               np.linspace(max_v, -max_v, half_pts, endpoint=False))), -half_pts // 2)\ncycles = [0, 1, 2]\nfields = [1, -1]\n\ndim_names = ['Bias', 'Field', 'Cycles']\n\nfig, axes = plt.subplots(ncols=3, figsize=(10, 3.5))\nfor axis, name, vec in zip(axes.flat, dim_names, [bi_triang, fields, cycles]):\n    axis.plot(vec, 'o-')\n    axis.set_title(name, fontsize=14)\nfig.suptitle('Unit values for each dimension', fontsize=16, y=1.05)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "make_indices_matrix()\n---------------------\nOne half of the work for generating the ancillary datasets is generating the ``indices matrix``. The\n``make_indices_matrix()`` function solves this problem. All one needs to do is supply a list with the lengths of each\ndimension. The result is a 2D matrix of shape: (3 dimensions, points in ``Bias``[``16``] * points in ``Field``[``2``] * points\nin ``Cycle``[``3``]) = ``(3, 96)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inds = usid.write_utils.make_indices_matrix([len(bi_triang), len(fields), len(cycles)], is_position=False)\nprint('Generated indices of shape: {}'.format(inds.shape))\n\n# The plots below show a visual representation of the indices for each dimension:\nfig, axes = plt.subplots(ncols=3, figsize=(10, 3.5))\nfor axis, name, vec in zip(axes.flat, dim_names, inds):\n    axis.plot(vec)\n    axis.set_title(name, fontsize=14)\nfig.suptitle('Indices for each dimension', fontsize=16, y=1.05)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "build_ind_val_matrices()\n--------------------------\n``make_indices_matrix()`` is a very handy function but it only solves one of the problems - the indices matrix. We also\nneed the matrix with the values tiled and repeated in the same manner. Perhaps one of the most useful functions is\n``build_ind_val_matrices()`` which uses just the values over which each dimension is varied to automatically generate\nthe indices and values matrices that form the ancillary datasets.\n\nIn order to generate the indices and values matrices, we would just need to provide the list of values over which\nthese dimensions are varied to ``build_ind_val_matrices()``. The results are two matrices - one for the indices and the\nother for the values, of the same shape ``(3, 96)``.\n\nAs mentioned in our document about the `data structuring <https://pycroscopy.github.io/pyUSID/data_format.html>`_,\nthe ``Bias`` would be in the first row, followed by ``Field``, finally followed by ``Cycle``. The plots below illustrate\nwhat the indices and values look like for each dimension. For example, notice how the bipolar triangular bias vector\nhas been repeated 2 (``Field``) * 3 (``Cycle``) times. Also note how the indices vector is a saw-tooth waveform that also\nrepeats in the same manner.  The repeated + tiled indices and values vectors for ``Cycle`` and ``Field`` look the same /\nvery similar since they were simple linearly increasing values to start with.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inds, vals = usid.write_utils.build_ind_val_matrices([bi_triang, fields, cycles], is_spectral=True)\nprint('Indices and values of shape: {}'.format(inds.shape))\n\nfig, axes = plt.subplots(ncols=3, figsize=(10, 3.5))\nfor axis, name, vec in zip(axes.flat, dim_names, inds):\n    axis.plot(vec)\n    axis.set_title(name, fontsize=14)\nfig.suptitle('Indices for each dimension', fontsize=16, y=1.05)\nfig.tight_layout()\n\nfig, axes = plt.subplots(ncols=3, figsize=(10, 3.5))\nfor axis, name, vec in zip(axes.flat, dim_names, vals):\n    axis.plot(vec)\n    axis.set_title(name, fontsize=14)\nfig.suptitle('Values for each dimension', fontsize=16, y=1.05)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create_spec_inds_from_vals()\n------------------------------\nWhen writing analysis functions or classes wherein one or more (typically spectroscopic) dimensions are dropped as a\nconsequence of dimensionality reduction, new ancillary spectroscopic datasets need to be generated when writing the\nreduced data back to the file. ``create_spec_inds_from_vals()`` is a handy function when we need to generate the indices\nmatrix that corresponds to a values matrix. For this example, lets assume that we only have the values matrix but need\nto generate the indices matrix from this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inds = usid.write_utils.create_spec_inds_from_vals(vals)\n\nfig, axes = plt.subplots(ncols=3, figsize=(10, 3.5))\nfor axis, name, vec in zip(axes.flat, dim_names, inds):\n    axis.plot(vec)\n    axis.set_title(name, fontsize=14)\nfig.suptitle('Indices for each dimension', fontsize=16, y=1.05)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get_aux_dset_slicing()\n------------------------\n``Region references`` are a handy feature in HDF5 that allow users to refer to a certain section of data within a\ndataset by name rather than the indices that define the region of interest.\n\nIn USID, we use region references to define the row or column in the ``ancillary dataset`` that corresponds to\neach dimension by its name. In other words, if we only wanted the ``Field`` dimension we could directly get the data\ncorresponding to this dimension without having to remember or figure out the index in which this dimension exists.\n\nLet's take the example of the Field dimension which occurs at index 1 in the ``(3, 96)`` shaped spectroscopic index /\nvalues matrix. We could extract the ``Field`` dimension from the 2D matrix by slicing each dimension using slice objects\n. We need the second row so the first dimension would be sliced as ``slice(start=1, stop=2)``. We need all the colummns\nin the second dimension so we would slice as ``slice(start=None, stop=None)`` meaning that we need all the columns.\nDoing this by hand for each dimension is clearly tedious.\n\n``get_aux_dset_slicing()`` helps generate the instructions for region references for each dimension in an ancillary\ndataset. The instructions for each region reference in ``h5py`` are defined by tuples of slice objects. Lets see the\nregion-reference instructions that this function provides for our aforementioned example dataset with the three\nspectroscopic dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Region references slicing instructions for Spectroscopic dimensions:')\nret_val = usid.write_utils.get_aux_dset_slicing(dim_names, is_spectroscopic=True)\nfor key, val in ret_val.items():\n    print('{} : {}'.format(key, val))\n\nprint('\\nRegion references slicing instructions for Position dimensions:')\nret_val = usid.write_utils.get_aux_dset_slicing(['X', 'Y'], is_spectroscopic=False)\nfor key, val in ret_val.items():\n    print('{} : {}'.format(key, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimension\n----------\nIn USID, ``position`` and ``spectroscopic dimensions`` are defined using some basic information that will be\nincorporated in ``Dimension`` objects that contain three vital pieces of information:\n\n* ``name`` of the dimension\n* ``units`` for the dimension\n* ``values``:\n    * These can be the actual values over which the dimension was varied\n    * or number of steps in case of linearly varying dimensions such as 'Cycle' below\n\nThese objects will be heavily used for creating ``Main`` or ``ancillary datasets`` in ``pyUSID.hdf_utils`` and even to\nset up interactive jupyter Visualizers in ``pyUSID.USIDataset``.\n\nNote that the ``Dimension`` objects in the lists for ``Position`` and ``Spectroscopic`` must be arranged from fastest\nvarying to slowest varying to mimic how the data is actually arranged. For example, in this example, there are\nmultiple bias points per field and multiple fields per cycle. Thus, the bias changes faster than the field and the\nfield changes faster than the cycle. Therefore, the ``Bias`` must precede ``Field`` which will precede ``Cycle``. Let's\nassume that we were describing the spectroscopic dimensions for this example dataset to some other pyUSID function\n, we would describe the spectroscopic dimensions as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spec_dims = [usid.write_utils.Dimension('Bias', 'V', bi_triang),\n             usid.write_utils.Dimension('Fields', '', fields),\n             # for the sake of example, since we know that cycles is linearly increasing from 0 with a step size of 1,\n             # we can specify such a simply dimension via just the length of that dimension:\n             usid.write_utils.Dimension('Cycle', '', len(cycles))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The application of the Dimension objects will be a lot more apparent in the document about the `writing functions in\npyUSID.hdf_utils <https://pycroscopy.github.io/pyUSID/auto_examples/cookbooks/plot_hdf_utils_write.html>`_.\n\nMisc writing utilities\n========================\n\ncalc_chunks()\n--------------\nThe ``h5py`` package automatically (virtually) breaks up HDF5 datasets into contiguous ``chunks`` to speed up reading and\nwriting of datasets. In certain situations the default mode of chunking may not result in the highest performance.\nIn such cases, it helps in chunking the dataset manually. The ``calc_chunks()`` function helps in calculating\nappropriate chunk sizes for the dataset using some apriori knowledge about the way the data would be accessed, the\nsize of each element in the dataset, maximum size for a single chunk, etc. The examples below illustrate a few ways on\nhow to use this function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dimensions = (16384, 16384 * 4)\ndtype_bytesize = 4\nret_val = usid.write_utils.calc_chunks(dimensions, dtype_bytesize)\nprint(ret_val)\n\ndimensions = (16384, 16384 * 4)\ndtype_bytesize = 4\nunit_chunks = (3, 7)\nmax_mem = 50000\nret_val = usid.write_utils.calc_chunks(dimensions, dtype_bytesize, unit_chunks=unit_chunks, max_chunk_mem=max_mem)\nprint(ret_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "clean_string_att()\n-------------------\nAs mentioned in our `HDF5 primer <https://pycroscopy.github.io/pyUSID/auto_examples/cookbooks/plot_h5py.html>`_,\nthe ``h5py`` package used for reading and manipulating HDF5 files has issues which necessitate the encoding of\nattributes whose values are lists of strings. The ``clean_string_att()`` encodes lists of\nstrings correctly so that they can directly be written to HDF5 without causing any errors. All other kinds of simple\nattributes - single strings, numbers, lists of numbers are unmodified by this function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expected = ['a', 'bc', 'def']\nreturned = usid.write_utils.clean_string_att(expected)\nprint('List of strings value: {} encoded to: {}'.format(expected, returned))\n\nexpected = [1, 2, 3.456]\nreturned = usid.write_utils.clean_string_att(expected)\nprint('List of numbers value: {} returned as is: {}'.format(expected, returned))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}