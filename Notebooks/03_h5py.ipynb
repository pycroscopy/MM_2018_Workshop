{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "================================================================================\n",
    "01. Primer to HDF5 and h5py\n",
    "================================================================================\n",
    "\n",
    "**Suhas Somnath**\n",
    "\n",
    "4/18/2018\n",
    "\n",
    "**This document serves as a quick primer to HDF5 files and the h5py package used for reading and writing to such files**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "-------------\n",
    "We create and consume digital information stored in various file formats on a daily basis such as news presented in\n",
    "HTML files, scientific journal articles in PDF files, tabular data in XLSX spreadsheets and so on. Commercially\n",
    "available scientific instruments generate data in a variety of, typically proprietary, file formats. The proprietary\n",
    "nature of the data impedes scientific research of individual researchers and the collaboration within the scientific\n",
    "community at large. Hence, pycroscopy stores all relevant information including the measurement data, metadata etc.\n",
    "in the most popular file format for scientific data - Hierarchical Data Format (HDF5) files.\n",
    "\n",
    "HDF5 is a remarkably straightforward file format to understand since it mimics the familiar folders and files paradigm\n",
    "exposed to users by all operating systems such as Windows, Mac OS, Linux, etc. HDF5 files can contain:\n",
    "\n",
    "* ``Datasets`` - similar to spreadsheets and text files with tabular data.\n",
    "* ``Groups`` - similar to folders in a regular file system\n",
    "* ``Attributes`` - small metadata that provide additional information about the Group or Dataset they are attached to.\n",
    "* other advanced features such as hard links, soft links, object and region references, etc.\n",
    "\n",
    "h5py is the official software package for reading and writing to HDF5 files in python. Consequently, Pycroscopy relies\n",
    "entirely on h5py for all file related operations. While there are several high-level functions that simplify the\n",
    "reading and writing of Pycroscopy stylized data, it is still crucial that the users of Pycroscopy understand the\n",
    "basics of HDF5 files and are familiar with the basic functions in h5py. There are several tutorials available\n",
    "elsewhere to explain h5py in great detail. This document serves as a quick primer to the basics of interacting with\n",
    "HDF5 files via h5py.\n",
    "\n",
    "Import all necessary packages\n",
    "-------------------------------\n",
    "For this primer, we only need some very basic packages, all of which come with the standard Anaconda distribution:\n",
    "\n",
    "* ``os`` - to manipulate and remove files\n",
    "* ``numpy`` - for basic numerical work\n",
    "* ``h5py`` - the package that will be the focus of this primer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wan-Yu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a HDF5 files using h5py is similar to the process of creating a conventional text file using python. The File\n",
    "class of h5py requires the path for the desired file with a .h5, .hdf5, or similar extension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"hdf5_primer.h5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "h5_path = 'hdf5_primer.h5'\n",
    "h5_file = h5py.File('hdf5_primer.h5')\n",
    "print(h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, a file in the path specified by h5_path has been created and is now open for modification. The returned\n",
    "value - h5_file is necessary to perform other operations on the file including creating groups and datasets.\n",
    "\n",
    "Groups\n",
    "===========\n",
    "create_group()\n",
    "----------------\n",
    "We can use the ``create_group()`` function on an existing object such as the open file handle (``h5_file``) to create a\n",
    "group:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Group_1\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "h5_group_1 = h5_file.create_group('Group_1')\n",
    "print(h5_group_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above print statement reveals that a group named ``Group_1`` was successfully created at location: '/'\n",
    "(which stands for the root of the file). Furthermore, this group contains 0 objects or members.\n",
    ".name\n",
    "-------\n",
    "One can find the full / absolute path where this object is located from its ``name`` property:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Group_1\n"
     ]
    }
   ],
   "source": [
    "print(h5_group_1.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups in Groups\n",
    "----------------\n",
    "Much like folders in a computer, these groups can themselves contain more groups and datasets.\n",
    "\n",
    "Let us create a few more groups the same way. Except, let us create these groups within the newly created. To do this,\n",
    "we would need to call the ``create_group()`` function on the h5_group_1 object and not the h5_file object. Doing the\n",
    "latter would result in groups created under the file at the same level as ``Group_1`` instead of inside ``Group_1``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_group_1_1 = h5_group_1.create_group('Group_1_1')\n",
    "h5_group_1_2 = h5_group_1.create_group('Group_1_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we print h5_group, it will reveal that we have two objects - the two groups we just created:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Group_1\" (2 members)>\n"
     ]
    }
   ],
   "source": [
    "print(h5_group_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what a similar print of one of the newly created groups looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Group_1/Group_1_1\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "print(h5_group_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above print statement shows that this group named ``Group_1_1`` exists at a path: ``\"/Group_1/Group_1_1\"``. In other\n",
    "words, this is similar to a folder contained inside another folder.\n",
    "\n",
    ".parent\n",
    "---------\n",
    "The hierarchical nature of HDF5 allows us to access datasets and groups using relationships or paths. For example,\n",
    "every HDF5 object has a parent. In the case of 'Group_1' - its parent is the root or h5_file itself. Similarly, the\n",
    "parent object of 'Group_1_1' is 'Group_1':\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent of \"Group_1\" is <HDF5 group \"/\" (1 members)>\n",
      "Parent of \"Group_1_1\" is <HDF5 group \"/Group_1\" (2 members)>\n"
     ]
    }
   ],
   "source": [
    "print('Parent of \"Group_1\" is {}'.format(h5_group_1.parent))\n",
    "print('Parent of \"Group_1_1\" is {}'.format(h5_group_1_1.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the .parent of an object is an HDF5 object (either a HDF5 group or HDF5 File object). So we can check if the\n",
    "parent of the h5_group_1_1 variable is indeed the h5_group_1 variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(h5_group_1_1.parent == h5_group_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing H5 objects\n",
    "----------------------\n",
    "Imagine a file or a folder on a computer that is several folders deep from where one is (e.g. -\n",
    "/Users/Joe/Documents/Projects/2018/pycroscopy).One could either reach the desired file or folder by opening one folder\n",
    "after another or directly by using a long path string. If you were at root (/), you would need to paste the entire\n",
    "path (absolute path) of the desired file -  ``/Users/Joe/Documents/Projects/2018/pycroscopy``. Alternatively, if you\n",
    "were in an intermediate directory (e.g. -  ``/Users/Joe/Documents/``), you would need to paste what is called the\n",
    "relative path (in this case -  ``Projects/2018/pycroscopy``) to get to the desired file.\n",
    "\n",
    "In the same way, we can also access HDF5 objects either through ``relative paths``, or ``absolute paths``. Here are a few\n",
    "ways one could get to the group ``Group_1_2``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Group_1/Group_1_2\" (0 members)>\n",
      "<HDF5 group \"/Group_1/Group_1_2\" (0 members)>\n",
      "<HDF5 group \"/Group_1/Group_1_2\" (0 members)>\n",
      "<HDF5 group \"/Group_1/Group_1_2\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "print(h5_file['/Group_1/Group_1_2'])\n",
    "print(h5_group_1['Group_1_2'])\n",
    "print(h5_group_1_1.parent['Group_1_2'])\n",
    "print(h5_group_1_1.parent.parent['Group_1/Group_1_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at how one can iterate through the datasets and Groups present within a HDF5 group:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group_1_1\n",
      "Group_1_2\n"
     ]
    }
   ],
   "source": [
    "for item in h5_group_1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".items()\n",
    "----------\n",
    "Essentially, h5py group objects contain a dictionary of key-value pairs where they key is the name of the object and\n",
    "the value is a reference to the object itself.\n",
    "\n",
    "What the above for loop does is it iterates only over the keys in this dictionary which are all strings. In order to\n",
    "get the actual dataset object itself, we would need to use the aforementioned addressing techniques to get the actual\n",
    "Group objects.\n",
    "\n",
    "Let us see how we would then try to find the object for the group named 'Group_1_2':\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the desired object: <HDF5 group \"/Group_1/Group_1_2\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "for key, value in h5_group_1.items():\n",
    "    if key == 'Group_1_2':\n",
    "        print('Found the desired object: {}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Group_1_1', <HDF5 group \"/Group_1/Group_1_1\" (0 members)>),\n",
       " ('Group_1_2', <HDF5 group \"/Group_1/Group_1_2\" (0 members)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(h5_group_1.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "===========\n",
    "create_dataset()\n",
    "----------------\n",
    "We can create a dataset within ``Group_1`` using a function that is similar to ``create_group()``, called\n",
    "``create_dataset()``. Unlike create_group() which just takes the path of the desired group as an input,\n",
    "``create_dataset()`` is highly customizable and flexible.\n",
    "\n",
    "In our experience, there are three modes of creating datasets that are highly relevant for scientific applications:\n",
    "\n",
    "* dataset with data at time of creation - where the data is already available at the time of creating the dataset\n",
    "* empty dataset - when one knows the size of data but the entire data is not available\n",
    "* resizable dataset - when one does not even know how large the data can be. *This case is rare*\n",
    "\n",
    "Creating Dataset with available data:\n",
    "-------------------------------------\n",
    "Let as assume we want to store a simple greyscale (floating point values) image with 256 x 256 pixels. We would create\n",
    "and store the data as shown below. As the size of the dataset becomes very large, the precision with which the data is\n",
    "stored can significantly affect the size of the dataset and the file. Therefore, we recommend purposefully specifying\n",
    "the data-type (via the ``dtype`` keyword argument) during creation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"Simple_Dataset\": shape (256, 256), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "h5_simple_dataset = h5_group_1.create_dataset('Simple_Dataset',\n",
    "                                              data=np.random.rand(256, 256),\n",
    "                                              dtype=np.float32)\n",
    "print(h5_simple_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data\n",
    "----------------\n",
    "We can access data contained in the dataset just like accessing a numpy array. For example, if we want the value at\n",
    "row ``29`` and column ``167``, we would read it as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45223424\n"
     ]
    }
   ],
   "source": [
    "print(h5_simple_dataset[29, 167])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just as before, we can address this dataset in many ways:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"Simple_Dataset\": shape (256, 256), type \"<f4\">\n",
      "<HDF5 dataset \"Simple_Dataset\": shape (256, 256), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "print(h5_group_1['Simple_Dataset'])\n",
    "print(h5_file['/Group_1/Simple_Dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "populating with data\n",
    "----------------------\n",
    "One could populate each chunk of the dataset just like filling in a numpy array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_simple_dataset[29, 167] = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.44570327+0.25381553j)\n",
      "0j\n"
     ]
    }
   ],
   "source": [
    "print(h5_simple_dataset[29, 167])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flush()\n",
    "--------\n",
    "It is a good idea to ensure that this data is indeed committed to the file using regular flush() operations. There are\n",
    "chances where the data is still in the memory / buffer and not yet in the file if one does not flush():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes\n",
    "===========\n",
    "* are metadata that can convey information that cannot be efficiently conveyed using Group or Dataset objects.\n",
    "* are almost exactly like python dictionaries in that they have a key-value pairs.\n",
    "* can be stored in either Group or Dataset objects.\n",
    "* are not appropriate for storing large amounts of information. Consider datasets instead\n",
    "* are best suited for things like experimental parameter such as beam intensity, scan rate, scan width, etc.\n",
    "\n",
    "Writing\n",
    "---------\n",
    "Storing attributes in objects is identical to appending to python dictionaries. Lets store some simple attributes in\n",
    "the group named 'Group_1':\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_simple_dataset.attrs['single_num'] = 36.23\n",
    "h5_simple_dataset.attrs.update({'list_of_nums': [1, 6.534, -65],\n",
    "                               'single_string': 'hello'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading\n",
    "----------\n",
    "We would read the attributes just like we would treat a dictionary in python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_num : 36.23\n",
      "list_of_nums : [  1.      6.534 -65.   ]\n",
      "single_string : hello\n"
     ]
    }
   ],
   "source": [
    "for key, val in h5_simple_dataset.attrs.items():\n",
    "    print('{} : {}'.format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read the attributes one by one and verify that we read what we wrote:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_num: True\n",
      "list_of_nums: True\n",
      "single_string: True\n"
     ]
    }
   ],
   "source": [
    "print('single_num: {}'.format(h5_simple_dataset.attrs['single_num'] == 36.23))\n",
    "print('list_of_nums: {}'.format(np.all(h5_simple_dataset.attrs['list_of_nums'] == [1, 6.534, -65])))\n",
    "print('single_string: {}'.format(h5_simple_dataset.attrs['single_string'] == 'hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "Once we are done reading or manipulating an HDF5 file, we need to close it to avoid and potential damage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_file.close()\n",
    "os.remove(h5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning this is not meant to be a comprehensive overview of HDF5 or h5py, but rather just a\n",
    "quick overview of the important functionality we recommend everyone to be familiar with. We encourage you to read more\n",
    "about h5py and HDF5 if you are interested.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
